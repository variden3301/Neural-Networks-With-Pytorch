{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e0e3234",
      "metadata": {
        "id": "3e0e3234"
      },
      "source": [
        "## Homework 2, Bahadır Erdem, 21070001048"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4256fb0",
      "metadata": {
        "id": "c4256fb0"
      },
      "source": [
        "A variation of the residual block called a bottleneck residual block uses 1x1 convolutions to produce a bottleneck. The number of parameters and matrix multiplications is decreased when a bottleneck is used. The goal is to reduce the number of factors and boost depth by making residual blocks as thin as possible. They are employed in deeper ResNets like ResNet-50 and ResNet-101 and were first included in the ResNet architecture.\n",
        "\n",
        "Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) while original implementation places the stride at the first 1x1 convolution(self.conv1) according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "\n",
        "The bottleneck residual block consists of three convolutional layers:\n",
        "\n",
        "1. **1x1 Convolution (Reduce dimension)**\n",
        "2. **3x3 Convolution (Process)**\n",
        "3. **1x1 Convolution (Restore dimension)**\n",
        "\n",
        "These layers are followed by batch normalization and ReLU activations. Additionally, a shortcut connection (residual connection) adds the input to the output of these layers, allowing the network to learn residual functions. Let's break down each component:\n",
        "\n",
        "1. **1x1 Convolution (Reduce Dimension)**\n",
        "   - **Input**: \\( x \\)\n",
        "   - **Convolution**: 1x1 convolution to reduce the number of channels to a smaller number (let’s call this \\( d \\)).\n",
        "   - **Batch Normalization**: Normalizes the output of the convolution.\n",
        "   - **ReLU Activation**: Applies the ReLU activation function.\n",
        "\n",
        "2. **3x3 Convolution (Process)**\n",
        "   - **Input**: Output from the previous step.\n",
        "   - **Convolution**: 3x3 convolution to process the reduced dimension.\n",
        "   - **Batch Normalization**: Normalizes the output of the convolution.\n",
        "   - **ReLU Activation**: Applies the ReLU activation function.\n",
        "\n",
        "3. **1x1 Convolution (Restore Dimension)**\n",
        "   - **Input**: Output from the previous step.\n",
        "   - **Convolution**: 1x1 convolution to restore the original dimension (number of channels).\n",
        "   - **Batch Normalization**: Normalizes the output of the convolution.\n",
        "\n",
        "4. **Residual Connection**\n",
        "   - The original input \\( x \\) is added to the output of the final batch normalization.\n",
        "   - **Addition**: \\( \\text{Output} = \\text{BatchNorm}_3 + x \\)\n",
        "\n",
        "5. **ReLU Activation**\n",
        "   - Applies the ReLU activation function to the final output.\n",
        "\n",
        "\n",
        "\n",
        "Source: https://doi.org/10.48550/arXiv.1512.03385"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "433467ec",
      "metadata": {
        "id": "433467ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c24632e-8cc9-4ca7-f736-ecfc29b19710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import models, transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load and prepare the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "trn_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "vld_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "tst_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training set into training and validation partitions\n",
        "trn_size = int(0.8 * len(trn_dataset))\n",
        "vld_size = len(trn_dataset) - trn_size\n",
        "torch.manual_seed(0)\n",
        "trn_dataset, vld_dataset = random_split(trn_dataset, [trn_size, vld_size])\n",
        "\n",
        "classes = 'Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck'\n",
        "num_classes = len(classes)\n",
        "\n",
        "batch_size = 128\n",
        "trn_loader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "vld_loader = DataLoader(vld_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "tst_loader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "def visualize_model_predictions2(model, loader=tst_loader, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: ' + classes[preds[j]])\n",
        "                plt.imshow(inputs.cpu().data[j])\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return model.train(mode=was_training)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below![indir.png](data:image/png;base64,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) is a rewritten version of visualise"
      ],
      "metadata": {
        "id": "16HWjyN2XtLT"
      },
      "id": "16HWjyN2XtLT"
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model_predictions(model, loader=tst_loader, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {classes[preds[j]]}')\n",
        "                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
        "                img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)\n",
        "                plt.imshow(img)\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "    model.train(mode=was_training)\n"
      ],
      "metadata": {
        "id": "o5JKaMu0ePPw"
      },
      "id": "o5JKaMu0ePPw",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This below is an example code for bottleneck\n",
        "I do not run this and it si purely for example purposes"
      ],
      "metadata": {
        "id": "0W5SdvIveO6t"
      },
      "id": "0W5SdvIveO6t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fde9ff",
      "metadata": {
        "id": "a2fde9ff"
      },
      "outputs": [],
      "source": [
        "#Original Resnet Bottleneck\n",
        "#For context. Taken from the resnet github.\n",
        "\n",
        "#NOT MY CODE\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "#DO NOT RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bottleneck Residual Block According to the specifications"
      ],
      "metadata": {
        "id": "7Eu7wEz6YL38"
      },
      "id": "7Eu7wEz6YL38"
    },
    {
      "cell_type": "code",
      "source": [
        "#Bottleneck Resudial Block\n",
        "\n",
        "class BottleneckResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BottleneckResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * 4:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "TP9MHDYuPEGQ"
      },
      "id": "TP9MHDYuPEGQ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBottleneck(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNetBottleneck, self).__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * 4\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet50(num_classes=10):\n",
        "    return ResNetBottleneck(BottleneckResidualBlock, [3, 4, 6, 3], num_classes=num_classes)"
      ],
      "metadata": {
        "id": "zBqi-jNtPgXc"
      },
      "id": "zBqi-jNtPgXc",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model for the first time here"
      ],
      "metadata": {
        "id": "X-TV4D5jYEy4"
      },
      "id": "X-TV4D5jYEy4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4 #I am not using this here with SGD but included it because it would be similiar looking for the adam with decay\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = resnet50(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "# Dataloaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "torch.manual_seed(0)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # L2 regularization term\n",
        "        l2_reg = 0\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        loss += weight_decay * l2_reg\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader.dataset))\n",
        "    train_acc.append(correct / total)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # L2 regularization term for validation (if needed)\n",
        "            l2_reg = 0\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param, 2)\n",
        "            loss += weight_decay * l2_reg\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader.dataset))\n",
        "    val_acc.append(correct / total)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}')\n",
        "\n",
        "# Plot learning and generalization curves\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning and Generalization Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # L2 regularization term for test (if needed)\n",
        "        l2_reg = 0\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        loss += weight_decay * l2_reg\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_acc = correct / total\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Visualize model predictions\n",
        "visualize_model_predictions(model, test_loader)\n"
      ],
      "metadata": {
        "id": "udk5ftIuPmb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "collapsed": true,
        "outputId": "4e579956-0146-41e1-ea38-1e3dadb6ef39"
      },
      "id": "udk5ftIuPmb9",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/20, Train Loss: 1.8294, Train Acc: 0.3491, Val Loss: 1.6795, Val Acc: 0.4255\n",
            "Epoch 2/20, Train Loss: 1.4090, Train Acc: 0.5189, Val Loss: 1.4585, Val Acc: 0.5098\n",
            "Epoch 3/20, Train Loss: 1.1367, Train Acc: 0.6234, Val Loss: 1.2238, Val Acc: 0.6053\n",
            "Epoch 4/20, Train Loss: 0.9378, Train Acc: 0.7003, Val Loss: 1.0962, Val Acc: 0.6555\n",
            "Epoch 5/20, Train Loss: 0.7712, Train Acc: 0.7571, Val Loss: 0.8881, Val Acc: 0.7244\n",
            "Epoch 6/20, Train Loss: 0.6541, Train Acc: 0.8024, Val Loss: 0.9876, Val Acc: 0.7200\n",
            "Epoch 7/20, Train Loss: 0.5532, Train Acc: 0.8363, Val Loss: 0.8747, Val Acc: 0.7368\n",
            "Epoch 8/20, Train Loss: 0.3259, Train Acc: 0.9252, Val Loss: 0.7121, Val Acc: 0.7940\n",
            "Epoch 9/20, Train Loss: 0.2502, Train Acc: 0.9531, Val Loss: 0.7259, Val Acc: 0.7949\n",
            "Epoch 10/20, Train Loss: 0.2113, Train Acc: 0.9680, Val Loss: 0.7503, Val Acc: 0.7920\n",
            "Epoch 11/20, Train Loss: 0.1797, Train Acc: 0.9790, Val Loss: 0.7754, Val Acc: 0.7906\n",
            "Epoch 12/20, Train Loss: 0.1565, Train Acc: 0.9878, Val Loss: 0.8036, Val Acc: 0.7877\n",
            "Epoch 13/20, Train Loss: 0.1394, Train Acc: 0.9925, Val Loss: 0.8176, Val Acc: 0.7874\n",
            "Epoch 14/20, Train Loss: 0.1271, Train Acc: 0.9960, Val Loss: 0.8480, Val Acc: 0.7887\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-99d75f59b9d6>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-db927bc0463d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9ea496609fb7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2510\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My observations are under the loss and accuracy data.\n",
        "I could not figure how to add pictures to collab without putting them to the drive so I will add it later.\n",
        "## After waiting for 44 minutes and 40 seconds the output is this:\n",
        "\n",
        "Epoch 1/20, Train Loss: 1.8294, Train Acc: 0.3491, Val Loss: 1.6795, Val Acc: 0.4255\n",
        "\n",
        "Epoch 2/20, Train Loss: 1.4090, Train Acc: 0.5189, Val Loss: 1.4585, Val Acc: 0.5098\n",
        "\n",
        "Epoch 3/20, Train Loss: 1.1367, Train Acc: 0.6234, Val Loss: 1.2238, Val Acc: 0.6053\n",
        "\n",
        "Epoch 4/20, Train Loss: 0.9378, Train Acc: 0.7003, Val Loss: 1.0962, Val Acc: 0.6555\n",
        "\n",
        "Epoch 5/20, Train Loss: 0.7712, Train Acc: 0.7571, Val Loss: 0.8881, Val Acc: 0.7244\n",
        "\n",
        "Epoch 6/20, Train Loss: 0.6541, Train Acc: 0.8024, Val Loss: 0.9876, Val Acc: 0.7200\n",
        "\n",
        "Epoch 7/20, Train Loss: 0.5532, Train Acc: 0.8363, Val Loss: 0.8747, Val Acc: 0.7368\n",
        "\n",
        "Epoch 8/20, Train Loss: 0.3259, Train Acc: 0.9252, Val Loss: 0.7121, Val Acc: 0.7940\n",
        "\n",
        "Epoch 9/20, Train Loss: 0.2502, Train Acc: 0.9531, Val Loss: 0.7259, Val Acc: 0.7949\n",
        "\n",
        "Epoch 10/20, Train Loss: 0.2113, Train Acc: 0.9680, Val Loss: 0.7503, Val Acc: 0.7920\n",
        "\n",
        "Epoch 11/20, Train Loss: 0.1797, Train Acc: 0.9790, Val Loss: 0.7754, Val Acc: 0.7906\n",
        "\n",
        "Epoch 12/20, Train Loss: 0.1565, Train Acc: 0.9878, Val Loss: 0.8036, Val Acc: 0.7877\n",
        "\n",
        "Epoch 13/20, Train Loss: 0.1394, Train Acc: 0.9925, Val Loss: 0.8176, Val Acc: 0.7874\n",
        "\n",
        "Epoch 14/20, Train Loss: 0.1271, Train Acc: 0.9960, Val Loss: 0.8480, Val Acc: 0.7887\n",
        "\n",
        "# My observations:\n",
        "\n",
        "I think there is a obvious overfitting starting about epoch 9 or 10 and it just stalls from there. Adding more regularization or augmentation would help it to achieve its full potential. Your results may varry because it is taking a lot of time and I just test and play with a smaller model to test things out. (Please forgive my poor writing I am very sleepy at the moment)"
      ],
      "metadata": {
        "id": "qUq3b4akrsXw"
      },
      "id": "qUq3b4akrsXw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different training dynamics.\n",
        "I am starting to the different training dynamics section.\n",
        "\n",
        "First ADAM with weight decay.\n",
        "\n",
        "I read this paper (briefly) to understand the differences between adam and sgd\n",
        "https://arxiv.org/pdf/2108.11371\n",
        "\n",
        "Which was pretty complicated and I don't think I understood.\n",
        "\n",
        "But my basic interpretation is as follows:\n",
        "Adam is an optimization algorithm that computes adaptive learning rates for each parameter. It combines the benefits of two other popular optimization methods: AdaGrad and RMSProp.\n",
        "\n",
        "Adam keeps an exponentially decaying average of past gradients and past squared gradients.\n"
      ],
      "metadata": {
        "id": "GgG2cVBhyUZU"
      },
      "id": "GgG2cVBhyUZU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "learning_rate = 0.001  # Lower learning rate for Adam\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = resnet50(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Dataloaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "torch.manual_seed(0)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader.dataset))\n",
        "    train_acc.append(correct / total)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader.dataset))\n",
        "    val_acc.append(correct / total)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}')\n",
        "\n",
        "# Plot learning and generalization curves\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning and Generalization Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_acc = correct / total\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Visualize model predictions\n",
        "visualize_model_predictions(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3jDU24pbh2b",
        "outputId": "2dd9e147-650b-4920-c1da-dd65359f657f"
      },
      "id": "I3jDU24pbh2b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/20, Train Loss: 1.5569, Train Acc: 0.4222, Val Loss: 1.6098, Val Acc: 0.4658\n",
            "Epoch 2/20, Train Loss: 1.1072, Train Acc: 0.6028, Val Loss: 1.0918, Val Acc: 0.6052\n",
            "Epoch 3/20, Train Loss: 0.8616, Train Acc: 0.6967, Val Loss: 1.0599, Val Acc: 0.6531\n",
            "Epoch 4/20, Train Loss: 0.6970, Train Acc: 0.7580, Val Loss: 0.7928, Val Acc: 0.7184\n",
            "Epoch 5/20, Train Loss: 0.5936, Train Acc: 0.7920, Val Loss: 0.8748, Val Acc: 0.6997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning rate scheduling"
      ],
      "metadata": {
        "id": "az1edJ1MZFbO"
      },
      "id": "az1edJ1MZFbO"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = resnet50(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Dataloaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "torch.manual_seed(0)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "learning_rates = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader.dataset))\n",
        "    train_acc.append(correct / total)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_losses.append(val_loss / len(val_loader.dataset))\n",
        "    val_acc.append(correct / total)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step(val_acc[-1])\n",
        "\n",
        "    # Track the current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "# Plot learning and generalization curves\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning and Generalization Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Plot learning rate evolution\n",
        "plt.figure()\n",
        "plt.plot(learning_rates, label='Learning Rate')\n",
        "plt.legend()\n",
        "plt.title('Learning Rate Evolution')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_acc = correct / total\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Visualize model predictions\n",
        "visualize_model_predictions(model, test_loader)\n"
      ],
      "metadata": {
        "id": "weB6oxTSrxxi"
      },
      "id": "weB6oxTSrxxi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}