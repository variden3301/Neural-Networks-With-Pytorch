{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 4437- Artificial Neural Network / 11.06.2024 / Course Project \n",
    "## Recognition and Classification of Celebrities Generated by AI\n",
    "\n",
    "### BahadÄ±r Erdem 21070001048 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project I am working with stylized portraits of celebrities made by artificial intelligence and my goal is the classify the celebrity and the style of the ai that made the portraits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "The dataset for this project includes training, validation, and test sets of celebrity caricatures. To enhance the quality and accuracy of the model, I applied data augmentation techniques to the training set. The validation and test sets were only resized to 224x224 pixels without augmentation to ensure consistent evaluation conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CelebCariDataset(root_dir='./Project/train', transform=train_transform)\n",
    "val_dataset = CelebCariDataset(root_dir='./Project/validation', transform=transform)\n",
    "test_dataset = CelebCariTestDataset(root_dir='./Project/test', transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "A simple multi-label classification model based on the pre-trained MobileNetV2 was employed. The model's backbone (all layers except the final classifier) was frozen to leverage pre-trained weights for feature extraction, and custom fully connected (FC) layers were added for identity and style classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiLabelModel(nn.Module):\n",
    "    def __init__(self, num_classes_identity, num_classes_style):\n",
    "        super(SimpleMultiLabelModel, self).__init__()\n",
    "        self.backbone = models.mobilenet_v2(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False  # Freeze the backbone\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.fc_identity = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes_identity)\n",
    "        )\n",
    "        self.fc_style = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes_style)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        identity_output = self.fc_identity(features)\n",
    "        style_output = self.fc_style(features)\n",
    "        return features, identity_output, style_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "```python\n",
    "SimpleMultiLabelModel(\n",
    "  (backbone): MobileNetV2(\n",
    "    (features): Sequential(\n",
    "      (0): Conv2dNormActivation(\n",
    "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU6(inplace=True)\n",
    "      )\n",
    "      (1): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
    "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (2): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
    "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (3): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
    "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (4): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
    "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (5): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (6): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (7): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
    "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (8): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (9): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (10): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (11): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
    "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (12): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (13): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (14): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
    "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (15): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (16): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (17): InvertedResidual(\n",
    "        (conv): Sequential(\n",
    "          (0): Conv2dNormActivation(\n",
    "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (1): Conv2dNormActivation(\n",
    "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
    "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU6(inplace=True)\n",
    "          )\n",
    "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (18): Conv2dNormActivation(\n",
    "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU6(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (classifier): Identity()\n",
    "  )\n",
    "  (fc_identity): Sequential(\n",
    "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=256, out_features=20, bias=True)\n",
    "  )\n",
    "  (fc_style): Sequential(\n",
    "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=256, out_features=6, bias=True)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure\n",
    "\n",
    "The training procedure included a custom loss function that combines identity and style classification losses with a weighted sum. An Adam optimizer with a higher weight decay (1e-3) to combat overfitting and a lower learning rate (0.001) was used. A learning rate scheduler was also implemented to reduce the learning rate by a factor of 0.1 every 5 epochs.\n",
    "\n",
    "Here is the links that I have done my research\n",
    "https://stackoverflow.com/questions/69763161/how-to-design-a-joint-loss-function-with-two-component-with-the-aim-of-minimizin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer with higher weight decay\n",
    "criterion_identity = nn.CrossEntropyLoss()\n",
    "criterion_style = nn.CrossEntropyLoss()\n",
    "\n",
    "def joint_loss(identity_output, style_output, identity_labels, style_labels, weight=0.5):\n",
    "    loss_identity = criterion_identity(identity_output, identity_labels)\n",
    "    loss_style = criterion_style(style_output, style_labels)\n",
    "    return weight * loss_identity + (1 - weight) * loss_style\n",
    "\n",
    "# Optimizer with higher weight decay and lower learning rate\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.fc_identity.parameters()},\n",
    "    {'params': model.fc_style.parameters()}\n",
    "], lr=0.001, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping and Model Evaluation\n",
    "\n",
    "Early stopping was implemented to prevent overfitting. The model's performance was monitored on the validation set, and training was stopped if there was no improvement in validation loss for 5 consecutive epochs. The best model weights (with the lowest validation loss) were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 7\n",
    "patience = 5\n",
    "\n",
    "# Training loop with early stopping and tracking\n",
    "best_model_wts = model.state_dict()\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Lists to store the loss and accuracy values\n",
    "train_losses, val_losses = [], []\n",
    "train_identity_losses, val_identity_losses = [], []\n",
    "train_style_losses, val_style_losses = [], []\n",
    "train_identity_accuracies, val_identity_accuracies = [], []\n",
    "train_style_accuracies, val_style_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "            dataloader = train_loader\n",
    "        else:\n",
    "            model.eval()\n",
    "            dataloader = val_loader\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_loss_identity = 0.0\n",
    "        running_loss_style = 0.0\n",
    "        correct_identity = 0\n",
    "        correct_style = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, identity_labels, style_labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            identity_labels = identity_labels.to(device)\n",
    "            style_labels = style_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                features, identity_output, style_output = model(inputs)\n",
    "                loss = joint_loss(identity_output, style_output, identity_labels, style_labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_loss_identity += criterion_identity(identity_output, identity_labels).item() * inputs.size(0)\n",
    "            running_loss_style += criterion_style(style_output, style_labels).item() * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "            _, predicted_identity = torch.max(identity_output, 1)\n",
    "            _, predicted_style = torch.max(style_output, 1)\n",
    "            \n",
    "            correct_identity += (predicted_identity == identity_labels).sum().item()\n",
    "            correct_style += (predicted_style == style_labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_loss_identity = running_loss_identity / total\n",
    "        epoch_loss_style = running_loss_style / total\n",
    "\n",
    "        accuracy_identity = correct_identity / total\n",
    "        accuracy_style = correct_style / total\n",
    "\n",
    "        if phase == 'train':\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_identity_losses.append(epoch_loss_identity)\n",
    "            train_style_losses.append(epoch_loss_style)\n",
    "            train_identity_accuracies.append(accuracy_identity)\n",
    "            train_style_accuracies.append(accuracy_style)\n",
    "        else:\n",
    "            val_losses.append(epoch_loss)\n",
    "            val_identity_losses.append(epoch_loss_identity)\n",
    "            val_style_losses.append(epoch_loss_style)\n",
    "            val_identity_accuracies.append(accuracy_identity)\n",
    "            val_style_accuracies.append(accuracy_style)\n",
    "\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "                torch.save(best_model_wts, 'best_model.pth')\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "```python\n",
    "Epoch 0/6\n",
    "----------\n",
    "train Loss: 1.3665\n",
    "train Identity Loss: 1.6406\n",
    "train Style Loss: 1.0925\n",
    "train Identity Accuracy: 0.5402\n",
    "train Style Accuracy: 0.6108\n",
    "val Loss: 2.0127\n",
    "val Identity Loss: 3.3435\n",
    "val Style Loss: 0.6819\n",
    "val Identity Accuracy: 0.0833\n",
    "val Style Accuracy: 0.7667\n",
    "Epoch 1/6\n",
    "----------\n",
    "train Loss: 0.6548\n",
    "train Identity Loss: 0.7660\n",
    "train Style Loss: 0.5436\n",
    "train Identity Accuracy: 0.7956\n",
    "train Style Accuracy: 0.8191\n",
    "val Loss: 2.4061\n",
    "val Identity Loss: 4.2438\n",
    "val Style Loss: 0.5683\n",
    "val Identity Accuracy: 0.1125\n",
    "val Style Accuracy: 0.7833\n",
    "Epoch 2/6\n",
    "----------\n",
    "train Loss: 0.5098\n",
    "train Identity Loss: 0.5337\n",
    "train Style Loss: 0.4858\n",
    "train Identity Accuracy: 0.8510\n",
    "train Style Accuracy: 0.8240\n",
    "val Loss: 2.3955\n",
    "val Identity Loss: 4.2900\n",
    "val Style Loss: 0.5010\n",
    "val Identity Accuracy: 0.0958\n",
    "val Style Accuracy: 0.8333\n",
    "Epoch 3/6\n",
    "----------\n",
    "train Loss: 0.4200\n",
    "train Identity Loss: 0.3979\n",
    "train Style Loss: 0.4422\n",
    "train Identity Accuracy: 0.8882\n",
    "train Style Accuracy: 0.8495\n",
    "val Loss: 2.6933\n",
    "val Identity Loss: 5.0021\n",
    "val Style Loss: 0.3845\n",
    "val Identity Accuracy: 0.0500\n",
    "val Style Accuracy: 0.8667\n",
    "Epoch 4/6\n",
    "----------\n",
    "train Loss: 0.3784\n",
    "train Identity Loss: 0.3563\n",
    "train Style Loss: 0.4005\n",
    "train Identity Accuracy: 0.8971\n",
    "train Style Accuracy: 0.8515\n",
    "val Loss: 2.9685\n",
    "val Identity Loss: 5.4769\n",
    "val Style Loss: 0.4600\n",
    "val Identity Accuracy: 0.0708\n",
    "val Style Accuracy: 0.8458\n",
    "Epoch 5/6\n",
    "----------\n",
    "train Loss: 0.2873\n",
    "train Identity Loss: 0.2725\n",
    "train Style Loss: 0.3021\n",
    "train Identity Accuracy: 0.9270\n",
    "train Style Accuracy: 0.9000\n",
    "val Loss: 2.8098\n",
    "val Identity Loss: 5.2538\n",
    "val Style Loss: 0.3658\n",
    "val Identity Accuracy: 0.0958\n",
    "val Style Accuracy: 0.8958\n",
    "Early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "After training, the loss and accuracy metrics for both identity and style classifications were plotted to visually assess the model's performance over the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(num_epochs)\n",
    "# Adjusting the lengths of epochs_range and metrics if needed\n",
    "min_length = min(len(epochs_range), len(train_losses), len(val_losses), \n",
    "                 len(train_identity_losses), len(val_identity_losses), \n",
    "                 len(train_style_losses), len(val_style_losses), \n",
    "                 len(train_identity_accuracies), len(val_identity_accuracies),\n",
    "                 len(train_style_accuracies), len(val_style_accuracies))\n",
    "\n",
    "# Truncate all lists to the minimum length\n",
    "epochs_range = epochs_range[:min_length]\n",
    "train_losses = train_losses[:min_length]\n",
    "val_losses = val_losses[:min_length]\n",
    "train_identity_losses = train_identity_losses[:min_length]\n",
    "val_identity_losses = val_identity_losses[:min_length]\n",
    "train_style_losses = train_style_losses[:min_length]\n",
    "val_style_losses = val_style_losses[:min_length]\n",
    "train_identity_accuracies = train_identity_accuracies[:min_length]\n",
    "val_identity_accuracies = val_identity_accuracies[:min_length]\n",
    "train_style_accuracies = train_style_accuracies[:min_length]\n",
    "val_style_accuracies = val_style_accuracies[:min_length]\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_identity_losses, label='Training Identity Loss')\n",
    "plt.plot(epochs_range, val_identity_losses, label='Validation Identity Loss')\n",
    "plt.plot(epochs_range, train_style_losses, label='Training Style Loss')\n",
    "plt.plot(epochs_range, val_style_losses, label='Validation Style Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Identity and Style Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss Graph](LossGraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Evaluation\n",
    "\n",
    "To evaluate the model on the validation set, embeddings were created for each image. A gallery of embeddings was maintained and updated based on similarity thresholds. Cosine similarity was used to compare embeddings, and a confusion matrix was generated to visualize the model's performance in classifying identities and styles.\n",
    "\n",
    "I couldn't make this part work well so I have gotten help from my friend Ege Bilge and I still do not understand why it works with two functions but whatever I tried to make it work didn't worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_and_update_gallery(model, val_dataset, gallery_json_path, device, similarity_threshold=0.5):\n",
    "    # Load gallery embeddings from JSON file\n",
    "    loaded_gallery_embeddings = read_gallery_from_json(gallery_json_path)\n",
    "    print(f\"Loaded gallery embeddings: {len(loaded_gallery_embeddings)} persons\")\n",
    "\n",
    "    # Create embeddings for the probe set (validation set)\n",
    "    model.eval()\n",
    "    probe_embeddings = {}\n",
    "    with torch.no_grad():\n",
    "        for images, person_labels, _ in DataLoader(val_dataset, batch_size=32, shuffle=False):\n",
    "            images = images.to(device)\n",
    "            embeddings, _, _ = model(images)\n",
    "            for embedding, person_label in zip(embeddings, person_labels):\n",
    "                person_name = val_dataset.index_to_person[person_label.item()]\n",
    "                if person_name not in probe_embeddings:\n",
    "                    probe_embeddings[person_name] = []\n",
    "                probe_embeddings[person_name].append(embedding.cpu())\n",
    "    \n",
    "    print(f\"Created probe embeddings: {len(probe_embeddings)} persons\")\n",
    "    total_val_images = sum(len(embeds) for embeds in probe_embeddings.values())\n",
    "    print(f\"Total validation images processed: {total_val_images}\")\n",
    "\n",
    "    # Ensure that we have the same number of images for validation and embedding creation\n",
    "    all_probe_embeddings = []\n",
    "    all_probe_labels = []\n",
    "    for person_name, embeddings in probe_embeddings.items():\n",
    "        all_probe_embeddings.extend(embeddings)\n",
    "        all_probe_labels.extend([person_name] * len(embeddings))\n",
    "\n",
    "    all_probe_tensors = torch.stack(all_probe_embeddings)\n",
    "    probe_tensors = torch.stack([torch.mean(torch.stack(embeds), dim=0) for embeds in probe_embeddings.values()])\n",
    "    gallery_tensors = torch.stack([torch.mean(torch.stack(embeds), dim=0) for embeds in loaded_gallery_embeddings.values()])\n",
    "\n",
    "    cos_sim = torch.matmul(probe_tensors, gallery_tensors.T)\n",
    "    cos_sim = cos_sim / (torch.norm(probe_tensors, dim=1, keepdim=True) * torch.norm(gallery_tensors, dim=1))\n",
    "\n",
    "    # Create a mapping from person names to indices\n",
    "    person_to_idx = {person: idx for idx, person in enumerate(loaded_gallery_embeddings.keys())}\n",
    "    idx_to_person = {idx: person for person, idx in person_to_idx.items()}\n",
    "\n",
    "    # Add an \"unknown\" category\n",
    "    unknown_idx = len(person_to_idx)\n",
    "    idx_to_person[unknown_idx] = \"unknown\"\n",
    "\n",
    "    # Determine predicted labels and handle unknown faces\n",
    "    predicted_indices = []\n",
    "    for i, similarities in enumerate(cos_sim):\n",
    "        max_similarity = torch.max(similarities).item()\n",
    "        person_name = list(probe_embeddings.keys())[i]\n",
    "        print(f\"Person: {person_name}, Max Similarity: {max_similarity}\")\n",
    "        if max_similarity < similarity_threshold:\n",
    "            predicted_indices.append(len(person_to_idx))  # Use new index for new person\n",
    "            if person_name not in loaded_gallery_embeddings:\n",
    "                print(f\"Adding {person_name} to gallery\")\n",
    "                loaded_gallery_embeddings[person_name] = probe_embeddings[person_name]\n",
    "                person_to_idx[person_name] = len(person_to_idx)\n",
    "                idx_to_person[len(idx_to_person)] = person_name\n",
    "        else:\n",
    "            predicted_indices.append(torch.argmax(similarities).item())\n",
    "\n",
    "    # True indices based on the probe set\n",
    "    true_indices = []\n",
    "    for person_name in all_probe_labels:\n",
    "        if person_name in person_to_idx:\n",
    "            person_idx = person_to_idx[person_name]\n",
    "        else:\n",
    "            person_idx = len(person_to_idx)  # Assign new index if not found\n",
    "            person_to_idx[person_name] = person_idx\n",
    "            idx_to_person[person_idx] = person_name\n",
    "            loaded_gallery_embeddings[person_name] = probe_embeddings[person_name]\n",
    "            print(f\"Added {person_name} to gallery as part of true_indices\")\n",
    "        true_indices.append(person_idx)\n",
    "\n",
    "    # Save updated gallery\n",
    "    save_gallery_to_json(loaded_gallery_embeddings, gallery_json_path)\n",
    "    print(f\"Updated gallery embeddings: {len(loaded_gallery_embeddings)} persons\")\n",
    "\n",
    "def evaluate_model_and_predict(model, val_dataset, gallery_json_path, device, similarity_threshold=0.5):\n",
    "    # Load gallery embeddings from JSON file\n",
    "    loaded_gallery_embeddings = read_gallery_from_json(gallery_json_path)\n",
    "    print(f\"Loaded gallery embeddings: {len(loaded_gallery_embeddings)} persons\")\n",
    "\n",
    "    # Create embeddings for the probe set (validation set)\n",
    "    model.eval()\n",
    "    probe_embeddings = {}\n",
    "    probe_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, person_labels, _ in DataLoader(val_dataset, batch_size=32, shuffle=False):\n",
    "            images = images.to(device)\n",
    "            embeddings, _, _ = model(images)\n",
    "            for embedding, person_label in zip(embeddings, person_labels):\n",
    "                person_name = val_dataset.index_to_person[person_label.item()]\n",
    "                if person_name not in probe_embeddings:\n",
    "                    probe_embeddings[person_name] = []\n",
    "                probe_embeddings[person_name].append(embedding.cpu())\n",
    "                probe_labels.append(person_name)\n",
    "\n",
    "    print(f\"Created probe embeddings: {len(probe_embeddings)} persons\")\n",
    "    total_val_images = sum(len(embeds) for embeds in probe_embeddings.values())\n",
    "    print(f\"Total validation images processed: {total_val_images}\")\n",
    "\n",
    "    # Ensure that we have the same number of images for validation and embedding creation\n",
    "    all_probe_embeddings = []\n",
    "    all_probe_labels = []\n",
    "    for person_name, embeddings in probe_embeddings.items():\n",
    "        all_probe_embeddings.extend(embeddings)\n",
    "        all_probe_labels.extend([person_name] * len(embeddings))\n",
    "\n",
    "    probe_tensors = torch.stack(all_probe_embeddings)\n",
    "    gallery_tensors = torch.stack([torch.mean(torch.stack(embeds), dim=0) for embeds in loaded_gallery_embeddings.values()])\n",
    "\n",
    "    cos_sim = torch.matmul(probe_tensors, gallery_tensors.T)\n",
    "    cos_sim = cos_sim / (torch.norm(probe_tensors, dim=1, keepdim=True) * torch.norm(gallery_tensors, dim=1))\n",
    "\n",
    "    # Create a mapping from person names to indices\n",
    "    person_to_idx = {person: idx for idx, person in enumerate(loaded_gallery_embeddings.keys())}\n",
    "    idx_to_person = {idx: person for person, idx in person_to_idx.items()}\n",
    "\n",
    "    # Add an \"unknown\" category\n",
    "    unknown_idx = len(person_to_idx)\n",
    "    idx_to_person[unknown_idx] = \"unknown\"\n",
    "\n",
    "    # Determine predicted labels and handle unknown faces\n",
    "    predicted_indices = []\n",
    "    for i, similarities in enumerate(cos_sim):\n",
    "        max_similarity = torch.max(similarities).item()\n",
    "        person_name = all_probe_labels[i]\n",
    "        print(f\"Person: {person_name}, Max Similarity: {max_similarity}\")\n",
    "        if max_similarity < similarity_threshold:\n",
    "            predicted_indices.append(unknown_idx)  # Use unknown index for unknown person\n",
    "            if person_name not in loaded_gallery_embeddings:\n",
    "                print(f\"Adding {person_name} to gallery\")\n",
    "                loaded_gallery_embeddings[person_name] = [probe_tensors[i]]\n",
    "                person_to_idx[person_name] = len(person_to_idx)\n",
    "                idx_to_person[len(idx_to_person)] = person_name\n",
    "        else:\n",
    "            predicted_indices.append(torch.argmax(similarities).item())\n",
    "\n",
    "    # True indices based on the probe set\n",
    "    true_indices = []\n",
    "    for person_name in all_probe_labels:\n",
    "        if person_name in person_to_idx:\n",
    "            person_idx = person_to_idx[person_name]\n",
    "        else:\n",
    "            person_idx = unknown_idx\n",
    "        true_indices.append(person_idx)\n",
    "\n",
    "    # Save updated gallery\n",
    "    save_gallery_to_json(loaded_gallery_embeddings, gallery_json_path)\n",
    "    print(f\"Updated gallery embeddings: {len(loaded_gallery_embeddings)} persons\")\n",
    "\n",
    "    if len(predicted_indices) != len(true_indices):\n",
    "        print(f\"Warning: Length mismatch - Predicted: {len(predicted_indices)}, True: {len(true_indices)}\")\n",
    "        min_length = min(len(predicted_indices), len(true_indices))\n",
    "        predicted_indices = predicted_indices[:min_length]\n",
    "        true_indices = true_indices[:min_length]\n",
    "\n",
    "    accuracy = (torch.tensor(predicted_indices) == torch.tensor(true_indices)).float().mean().item()\n",
    "    print(f'Validation Set Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    true_labels = [idx_to_person[idx] for idx in true_indices]\n",
    "    predicted_labels = [idx_to_person[idx] for idx in predicted_indices]\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=list(idx_to_person.values()))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=idx_to_person.values(), yticklabels=idx_to_person.values())\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    true_count = pd.Series(true_labels).value_counts().sort_index()\n",
    "    pred_count = pd.Series(predicted_labels).value_counts().sort_index()\n",
    "\n",
    "    if not true_count.empty and not pred_count.empty:\n",
    "        true_count.plot(kind='bar', alpha=0.5, color='blue', position=0, label='True Labels')\n",
    "        pred_count.plot(kind='bar', alpha=0.5, color='red', position=1, label='Predicted Labels')\n",
    "\n",
    "        plt.xlabel('Labels')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('True vs Predicted Labels')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No data to plot.\")\n",
    "\n",
    "# Call the evaluation functions\n",
    "gallery_json_path = 'gallery_embeddings.json' \n",
    "evaluate_model_and_update_gallery(model, val_dataset, gallery_json_path, device)\n",
    "evaluate_model_and_predict(model, val_dataset, gallery_json_path, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that I managed to get the celebrity classification working but I could not make the styles part.\n",
    "\n",
    "And this is the celebrity conf matrix that I got.\n",
    "\n",
    "![Celebrity Matrix](matrixceleb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
