{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to see if gpu is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU is available')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, datasets\n",
    "from helper import (CelebCariDataset, CelebCariTestDataset, calculate_accuracy, save_gallery_to_json, create_gallery_embeddings, read_gallery_from_json, encode_embedding, write_predictions_to_json)\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and dataloaders with data augmentation for the training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# No augmentation for validation and test sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CelebCariDataset(root_dir='./Project/train', transform=train_transform)\n",
    "val_dataset = CelebCariDataset(root_dir='./Project/validation', transform=transform)\n",
    "test_dataset = CelebCariTestDataset(root_dir='./Project/test', transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yaşar\\Yazılım\\Neural-Networks-With-Pytorch\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Yaşar\\Yazılım\\Neural-Networks-With-Pytorch\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     61\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, (identity_labels, style_labels) \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     63\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     64\u001b[0m     identity_labels \u001b[38;5;241m=\u001b[39m identity_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiLabelModel(nn.Module):\n",
    "    def __init__(self, num_identities, num_styles):\n",
    "        super(MultiLabelModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Capture the number of features in the last layer of the backbone\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Identity prediction head\n",
    "        self.identity_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_identities)\n",
    "        )\n",
    "        \n",
    "        # Style prediction head\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_styles)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        identity_logits = self.identity_head(features)\n",
    "        style_logits = self.style_head(features)\n",
    "        return features, identity_logits, style_logits\n",
    "\n",
    "# Instantiate the model and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_identities = 12  # Replace with the actual number of identities in your dataset\n",
    "num_styles = 6  # Replace with the actual number of styles in your dataset\n",
    "model = MultiLabelModel(num_identities, num_styles).to(device)\n",
    "\n",
    "# Define loss functions\n",
    "identity_criterion = nn.CrossEntropyLoss()\n",
    "style_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def multi_label_loss(identity_logits, style_logits, identity_labels, style_labels):\n",
    "    identity_loss = identity_criterion(identity_logits, identity_labels)\n",
    "    style_loss = style_criterion(style_logits, style_labels)\n",
    "    return identity_loss + style_loss\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# Create a training dataset instance\n",
    "train_dataset = CelebCariDataset(root_dir='./Project/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25  # Adjust based on your needs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, (identity_labels, style_labels) in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        identity_labels = identity_labels.to(device)\n",
    "        style_labels = style_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        features, identity_logits, style_logits = model(inputs)\n",
    "        loss = multi_label_loss(identity_logits, style_logits, identity_labels, style_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Implement validation accuracy calculation here\n",
    "    pass\n",
    "\n",
    "# Save gallery embeddings\n",
    "gallery_embeddings = create_gallery_embeddings(model, train_dataset, device)\n",
    "save_gallery_to_json(gallery_embeddings, 'gallery_embeddings.json')\n",
    "\n",
    "# Create a test dataset instance\n",
    "test_dataset = CelebCariTestDataset(root_dir='./Project/test', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load gallery embeddings\n",
    "loaded_gallery_embeddings = read_gallery_from_json('gallery_embeddings.json')\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, image_paths in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        embeddings, identity_logits, style_logits = model(inputs)\n",
    "        for embedding, image_path in zip(embeddings, image_paths):\n",
    "            # Perform identity recognition using cosine similarity with gallery embeddings\n",
    "            # Dummy style prediction for illustration; replace with actual prediction logic\n",
    "            style_prediction = torch.argmax(style_logits, dim=1)\n",
    "            predictions.append((image_path, embedding, style_prediction.item()))\n",
    "\n",
    "# Write predictions to a JSON file\n",
    "write_predictions_to_json(predictions, 'test_predictions.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
