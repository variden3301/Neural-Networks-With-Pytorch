{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 , BahadÄ±r Erdem , 21070001048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Normalization by Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For broadcasting: https://www.youtube.com/watch?v=tuKHsfAehz4\n",
    "* Broadcasting enables operations to be performed element-wise on arrays of different shapes without the need for the arrays to have the same shape. \n",
    "* Broadcasting is useful when we want to perform operations between arrays of different shapes without explicitly reshaping or duplicating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries that I use in this project\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n",
      "[[ 1  2  3]\n",
      " [ 2  4  6]\n",
      " [ 3  6  9]\n",
      " [ 4  8 12]\n",
      " [ 5 10 15]\n",
      " [ 6 12 18]\n",
      " [ 7 14 21]\n",
      " [ 8 16 24]\n",
      " [ 9 18 27]\n",
      " [10 20 30]]\n",
      "[[-1.5666989  -1.5666989  -1.5666989 ]\n",
      " [-1.21854359 -1.21854359 -1.21854359]\n",
      " [-0.87038828 -0.87038828 -0.87038828]\n",
      " [-0.52223297 -0.52223297 -0.52223297]\n",
      " [-0.17407766 -0.17407766 -0.17407766]\n",
      " [ 0.17407766  0.17407766  0.17407766]\n",
      " [ 0.52223297  0.52223297  0.52223297]\n",
      " [ 0.87038828  0.87038828  0.87038828]\n",
      " [ 1.21854359  1.21854359  1.21854359]\n",
      " [ 1.5666989   1.5666989   1.5666989 ]]\n",
      "Mean: [-1.11022302e-16 -1.11022302e-16 -1.11022302e-16]\n",
      "Std: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def stdnorm(X):\n",
    "    return (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True)\n",
    "N = 10\n",
    "D = 3\n",
    "x_org = np.arange(1, N+1).reshape(N,1) @ np.arange(1, D+1).reshape(1,D)\n",
    "x_org = np.arange(1, N+1)[:, np.newaxis] * np.arange(1, D+1)   #I am not sure about this line. I will look it again later.\n",
    "print(x_org.shape)\n",
    "print(x_org)\n",
    "x_norm = stdnorm(x_org)\n",
    "print(x_norm)\n",
    "print('Mean:', np.mean(x_norm, axis=0))\n",
    "print('Std:', np.std(x_norm, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The XOR Problem\n",
    "Design and train a neural network with the minimum number of\n",
    "neurons that predicts XOR outputs with 100% accuracy. If you use more neurons than the minimum\n",
    "number you get 5 points less. You must clearly show that your model works. Create the XOR dataset\n",
    "yourself. You are free to apply any learning method (shortly explain what you apply), but you MUST\n",
    "use only numpy for the computations. No other package is allowed for the computations, you can use\n",
    "other packages only for visualization or reporting. You are free to use any code from the lab sessions.\n",
    "\n",
    "This website helped a lot: https://www.sharpsightlabs.com/blog/python-perceptron-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,4) and (2,1) not aligned: 4 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m TwoLayerMLP(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Plot the loss curve\u001b[39;00m\n\u001b[0;32m    104\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "Cell \u001b[1;32mIn[1], line 89\u001b[0m, in \u001b[0;36mTwoLayerMLP.train\u001b[1;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     87\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weights(learning_rate)\n",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m, in \u001b[0;36mTwoLayerMLP.backward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdW2 \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH1\u001b[38;5;241m.\u001b[39mT, dZ2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_lambda \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m dZ1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ1)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdW1 \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, dZ1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_lambda \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,4) and (2,1) not aligned: 4 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# The XOR Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "class TwoLayerMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, reg_lambda):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.init_weights()\n",
    "        self.zero_grad()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dW1 = np.zeros_like(self.W1)\n",
    "        self.db1 = np.zeros_like(self.b1)\n",
    "        self.dW2 = np.zeros_like(self.W2)\n",
    "        self.db2 = np.zeros_like(self.b2)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = self.sigmoid(Z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward(self, X, Y=None):\n",
    "        self.X = X\n",
    "        if Y is not None:\n",
    "            self.Y = Y\n",
    "\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.H1 = self.sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.H1, self.W2) + self.b2\n",
    "        self.H2 = self.Z2\n",
    "        return self.H2\n",
    "\n",
    "    def compute_loss(self, Y_pred=None, Y=None):\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.H2\n",
    "        if Y is None:\n",
    "            Y = self.Y\n",
    "\n",
    "        batch_size = Y_pred.shape[0]\n",
    "        data_loss = np.sum((Y_pred - Y) ** 2) / batch_size\n",
    "        reg_loss = (self.reg_lambda / 2) * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2))) / batch_size\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        Y_pred = self.H2\n",
    "        Y = self.Y\n",
    "        X = self.X\n",
    "\n",
    "        dZ2 = (Y_pred - Y) * self.sigmoid_derivative(self.Z2)\n",
    "        self.dW2 = (np.dot(self.H1.T, dZ2) + self.reg_lambda * self.W2)\n",
    "        self.db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dZ1 = np.dot(dZ2, self.W2) * self.sigmoid_derivative(self.Z1)\n",
    "        self.dW1 = (np.dot(X.T, dZ1) + self.reg_lambda * self.W1)\n",
    "        self.db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            self.forward(X, y)\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss()\n",
    "            losses.append(loss)\n",
    "            # Backward pass\n",
    "            self.backward()\n",
    "            # Update weights\n",
    "            self.update_weights(learning_rate)\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.forward(X) >= 0.5, 1, 0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = TwoLayerMLP(input_size=2, hidden_size=2, output_size=1, reg_lambda=0.01)\n",
    "\n",
    "# Train the model\n",
    "losses = model.train(X, y, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate accuracy\n",
    "predictions = model.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the accuracy with the epochs\n",
    "plt.plot(np.arange(100), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
